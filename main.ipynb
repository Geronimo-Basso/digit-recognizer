{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is. For every in the test set, you should predict the correct label.",
   "id": "79247ce6f5bd4c20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:25.744354Z",
     "start_time": "2024-08-01T23:07:25.741109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structure\n",
    "1. Data exploration.\n",
    "2. Feature engineering.\n",
    "3. Data Preprocessing for Model.\n",
    "4. Basic model building.\n",
    "5. Model tunning.\n",
    "6. Ensamble model building.\n",
    "7. Results."
   ],
   "id": "88c5ced9750c96fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:27.665640Z",
     "start_time": "2024-08-01T23:07:25.800587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "print(f\"Training size {training.shape}.\")\n",
    "print(f\"Test size {test.shape}.\")"
   ],
   "id": "5ecf01174fb0df32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size (42000, 785).\n",
      "Test size (28000, 784).\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:27.683294Z",
     "start_time": "2024-08-01T23:07:27.667131Z"
    }
   },
   "cell_type": "code",
   "source": "training.info()",
   "id": "823dbec75c73751a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:27.702298Z",
     "start_time": "2024-08-01T23:07:27.684363Z"
    }
   },
   "cell_type": "code",
   "source": "print(training.isnull().sum()) # No null values, so no need to think how to manage them. ",
   "id": "4a621152e4e07b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label       0\n",
      "pixel0      0\n",
      "pixel1      0\n",
      "pixel2      0\n",
      "pixel3      0\n",
      "           ..\n",
      "pixel779    0\n",
      "pixel780    0\n",
      "pixel781    0\n",
      "pixel782    0\n",
      "pixel783    0\n",
      "Length: 785, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:27.797963Z",
     "start_time": "2024-08-01T23:07:27.704397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_labels = training['label']\n",
    "plt.hist(df_labels)"
   ],
   "id": "3c33a6b8416e3b58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4132., 4684., 4177., 4351., 4072., 3795., 4137., 4401., 4063.,\n",
       "        4188.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:27.801247Z",
     "start_time": "2024-08-01T23:07:27.798846Z"
    }
   },
   "cell_type": "code",
   "source": "# The digits we have information to predict are pretty equal in terms of quantity, except number '1'. It has more but not crazy more.",
   "id": "f4ef8f70cd1a8d12",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing for Model.\n",
   "id": "9cd4268b860f01fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:27.967889Z",
     "start_time": "2024-08-01T23:07:27.802208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = training.drop('label', axis=1)\n",
    "y = training['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'X_train size: {X_train.shape}')\n",
    "print(f'X_test size: {X_test.shape}')"
   ],
   "id": "1596f05ffff2124f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (33600, 784)\n",
      "X_test size: (8400, 784)\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basic model building.",
   "id": "1a3187a5136cb04e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:44.987571Z",
     "start_time": "2024-08-01T23:07:27.969101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    Dense(units=150, activation='relu', name='L1'),\n",
    "    Dense(units=75, activation='relu', name='L2'),\n",
    "    Dense(units=10, activation='linear', name='L3'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")"
   ],
   "id": "76886fda1bf9db54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.7937 - loss: 6.3416\n",
      "Epoch 2/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9205 - loss: 0.4936\n",
      "Epoch 3/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9392 - loss: 0.2876\n",
      "Epoch 4/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9491 - loss: 0.2158\n",
      "Epoch 5/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9498 - loss: 0.2016\n",
      "Epoch 6/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9598 - loss: 0.1513\n",
      "Epoch 7/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9589 - loss: 0.1402\n",
      "Epoch 8/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9640 - loss: 0.1203\n",
      "Epoch 9/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9677 - loss: 0.1130\n",
      "Epoch 10/10\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step - accuracy: 0.9759 - loss: 0.0855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3530bfc10>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:46.008390Z",
     "start_time": "2024-08-01T23:07:44.988442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "# Evaluating the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ],
   "id": "dc2569c771a3033",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9753\n",
      "Test Accuracy: 0.9521\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Complex model building ",
   "id": "49b9977ba9b83212"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Learning: \n",
    "\n",
    "The ImageDataGenerator doesn't apply all transformations to every image, nor does it choose just one. Instead, it applies a random combination of the specified transformations to each image, with each transformation applied independently. Here's how it works:\n",
    "1. For each image, the generator randomly decides whether to apply each transformation.\n",
    "2. If a transformation is applied, its magnitude is randomly chosen within the specified range.\n"
   ],
   "id": "dfe8667adba2bbcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data augmentation",
   "id": "176684a3c327c825"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:46.014705Z",
     "start_time": "2024-08-01T23:07:46.009544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_dataset_1(X, y, num_augmented_per_image=1):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating transformed versions of each image.\n",
    "    \n",
    "    Args:\n",
    "    X (numpy.ndarray): Input images, shape (n_samples, 784)\n",
    "    y (numpy.ndarray): Labels, shape (n_samples,)\n",
    "    num_augmented_per_image (int): Number of augmented versions to create for each original image\n",
    "    plot_examples (bool): If True, plot examples of original and augmented images\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_combined, y_combined) - Augmented dataset and corresponding labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape X from (n_samples, 784) to (n_samples, 28, 28, 1)\n",
    "    X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,          # Rotate images up to 10 degrees.\n",
    "        zoom_range=0.1,             # Zoom in or out up to 10%. \n",
    "        width_shift_range=0.1,      # Shift horizontally up to 10% of the width.\n",
    "        height_shift_range=0.1,     # Shift vertically up to 10% of the width\n",
    "        shear_range=0.1,            #  Apply shear transformations up to 10%.\n",
    "        fill_mode='nearest'         # Fill in newly created pixels with the nearest pixel value. \n",
    "    )\n",
    "\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        img = X[i]\n",
    "        label = y[i]\n",
    "        img = img.reshape((1,) + img.shape)  # reshape for datagen\n",
    "        \n",
    "        j = 0\n",
    "        for batch in datagen.flow(img, batch_size=1):\n",
    "            X_augmented.append(batch[0])\n",
    "            y_augmented.append(label)\n",
    "            j += 1\n",
    "            if j >= num_augmented_per_image:\n",
    "                break\n",
    "\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    y_augmented = np.array(y_augmented)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    X_combined = np.concatenate((X, X_augmented), axis=0)\n",
    "    y_combined = np.concatenate((y, y_augmented), axis=0)\n",
    "\n",
    "    # Flatten the images back to (n_samples, 784)\n",
    "    X_combined = X_combined.reshape(X_combined.shape[0], -1)\n",
    "\n",
    "    return X_combined, y_combined"
   ],
   "id": "648051b55182aca9",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:46.021946Z",
     "start_time": "2024-08-01T23:07:46.017802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_augmented_examples(X_original, X_augmented, num_examples=25):\n",
    "    \"\"\"\n",
    "    Plot examples of original and augmented images side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_examples, 2, figsize=(10, num_examples * 5))\n",
    "    for i in range(num_examples):\n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(X_original[i].reshape(28, 28), cmap='gray')\n",
    "        axes[i, 0].set_title('Original')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot augmented image\n",
    "        axes[i, 1].imshow(X_augmented[i].reshape(28, 28), cmap='gray')\n",
    "        axes[i, 1].set_title('Augmented')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "503b25028fa2e770",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:46.027745Z",
     "start_time": "2024-08-01T23:07:46.023211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_dataset_2(X, y, num_augmented_per_image=1, plot_examples=False):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating transformed versions of each image.\n",
    "    \n",
    "    Args:\n",
    "    X (numpy.ndarray): Input images, shape (n_samples, 784)\n",
    "    y (numpy.ndarray): Labels, shape (n_samples,)\n",
    "    num_augmented_per_image (int): Number of augmented versions to create for each original image\n",
    "    plot_examples (bool): If True, plot examples of original and augmented images\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_combined, y_combined) - Augmented dataset and corresponding labels\n",
    "    \"\"\"\n",
    "    # Reshape X from (n_samples, 784) to (n_samples, 28, 28, 1)\n",
    "    X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "    # Create an ImageDataGenerator instance\n",
    "    global datagen\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        zoom_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        img = X[i]\n",
    "        label = y[i]\n",
    "        img = img.reshape((1,) + img.shape)  # reshape for datagen\n",
    "        \n",
    "        j = 0\n",
    "        for batch in datagen.flow(img, batch_size=1):\n",
    "            X_augmented.append(batch[0])\n",
    "            y_augmented.append(label)\n",
    "            j += 1\n",
    "            if j >= num_augmented_per_image:\n",
    "                break\n",
    "\n",
    "    X_augmented = np.array(X_augmented)\n",
    "    y_augmented = np.array(y_augmented)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    X_combined = np.concatenate((X, X_augmented), axis=0)\n",
    "    y_combined = np.concatenate((y, y_augmented), axis=0)\n",
    "\n",
    "    # Flatten the images back to (n_samples, 784)\n",
    "    X_combined = X_combined.reshape(X_combined.shape[0], -1)\n",
    "\n",
    "    if plot_examples:\n",
    "        plot_augmented_examples(X, X_augmented)\n",
    "\n",
    "    return X_combined, y_combined"
   ],
   "id": "638860d75384bd5e",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:07:46.297365Z",
     "start_time": "2024-08-01T23:07:46.028971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = training.drop('label', axis=1).values\n",
    "y = training['label'].values\n",
    "\n",
    "# ---- Comment this to stop doing data augmentation -----\n",
    "#X, y = augment_dataset(X, y, num_augmented_per_image=1)\n",
    "# X, y = augment_dataset_2(X, y, num_augmented_per_image=1, plot_examples=True)\n",
    "# print(f\"Length X and y train: {X.shape} {y.shape}\")\n",
    "\n",
    "# Reshape the data to fit the CNN input\n",
    "X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "af0cbca466f0edf2",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:13:17.004011Z",
     "start_time": "2024-08-01T23:07:46.298345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ],
   "id": "fc40202f28a872b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geronimobasso/Desktop/extra/kaggle-competitions/titanic/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 15ms/step - accuracy: 0.8452 - loss: 0.5093 - val_accuracy: 0.9793 - val_loss: 0.0742\n",
      "Epoch 2/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 16ms/step - accuracy: 0.9651 - loss: 0.1155 - val_accuracy: 0.9835 - val_loss: 0.0518\n",
      "Epoch 3/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 17ms/step - accuracy: 0.9711 - loss: 0.0909 - val_accuracy: 0.9807 - val_loss: 0.0598\n",
      "Epoch 4/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 16ms/step - accuracy: 0.9765 - loss: 0.0750 - val_accuracy: 0.9863 - val_loss: 0.0420\n",
      "Epoch 5/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9786 - loss: 0.0682 - val_accuracy: 0.9899 - val_loss: 0.0332\n",
      "Epoch 6/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9796 - loss: 0.0644 - val_accuracy: 0.9856 - val_loss: 0.0438\n",
      "Epoch 7/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9822 - loss: 0.0569 - val_accuracy: 0.9904 - val_loss: 0.0314\n",
      "Epoch 8/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9834 - loss: 0.0533 - val_accuracy: 0.9892 - val_loss: 0.0364\n",
      "Epoch 9/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9861 - loss: 0.0448 - val_accuracy: 0.9893 - val_loss: 0.0337\n",
      "Epoch 10/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9865 - loss: 0.0449 - val_accuracy: 0.9919 - val_loss: 0.0263\n",
      "Epoch 11/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 16ms/step - accuracy: 0.9865 - loss: 0.0449 - val_accuracy: 0.9904 - val_loss: 0.0308\n",
      "Epoch 12/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 16ms/step - accuracy: 0.9871 - loss: 0.0436 - val_accuracy: 0.9911 - val_loss: 0.0289\n",
      "Epoch 13/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 17ms/step - accuracy: 0.9888 - loss: 0.0351 - val_accuracy: 0.9918 - val_loss: 0.0248\n",
      "Epoch 14/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 16ms/step - accuracy: 0.9883 - loss: 0.0349 - val_accuracy: 0.9932 - val_loss: 0.0243\n",
      "Epoch 15/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 16ms/step - accuracy: 0.9884 - loss: 0.0370 - val_accuracy: 0.9905 - val_loss: 0.0316\n",
      "Epoch 16/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9903 - loss: 0.0318 - val_accuracy: 0.9929 - val_loss: 0.0229\n",
      "Epoch 17/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 15ms/step - accuracy: 0.9887 - loss: 0.0355 - val_accuracy: 0.9927 - val_loss: 0.0235\n",
      "Epoch 18/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 16ms/step - accuracy: 0.9903 - loss: 0.0309 - val_accuracy: 0.9917 - val_loss: 0.0249\n",
      "Epoch 19/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 16ms/step - accuracy: 0.9900 - loss: 0.0304 - val_accuracy: 0.9932 - val_loss: 0.0240\n",
      "Epoch 20/20\n",
      "\u001B[1m1050/1050\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 17ms/step - accuracy: 0.9907 - loss: 0.0294 - val_accuracy: 0.9940 - val_loss: 0.0218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3538b52d0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:24.280575Z",
     "start_time": "2024-08-01T23:15:19.977200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ],
   "id": "f206642d97f2e726",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9988\n",
      "Test Accuracy: 0.9940\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:16:03.390726Z",
     "start_time": "2024-08-01T23:16:03.374403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtain predictions on the test set\n",
    "predictions = model.predict(X_val)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Generate and plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "id": "b91a16298fc2765c",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[174], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Obtain predictions on the test set\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m(X_val)\n\u001B[1;32m      3\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(predictions, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Generate and plot the confusion matrix\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'predict'"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Make upload predictions",
   "id": "e1e7d3623229ec67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:37.296972Z",
     "start_time": "2024-08-01T23:15:33.975504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = test.values\n",
    "\n",
    "# Reshape the data to fit the CNN input\n",
    "X = X.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "predictions = model.predict(X) # the predictions values are not final, we need to apply softmax again in order to change them to probability."
   ],
   "id": "3fa281dba4e063da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m875/875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 3ms/step\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:37.300923Z",
     "start_time": "2024-08-01T23:15:37.298316Z"
    }
   },
   "cell_type": "code",
   "source": "print(predictions[0])",
   "id": "65c5fd8fad642d58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4545167e-08 1.6132262e-09 9.9999774e-01 2.7396887e-08 7.3657780e-08\n",
      " 1.3640887e-09 1.5548217e-10 1.8326812e-06 1.5283629e-07 6.4723409e-08]\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:37.305095Z",
     "start_time": "2024-08-01T23:15:37.301816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sm_preferred = tf.nn.softmax(predictions).numpy() # Now with this the values are probabilities, we need to do a final change to get the index with the highes probability in order to get the final predictions.\n",
    "print(sm_preferred[0])"
   ],
   "id": "6128a4e1d6ea7b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08533677 0.08533677 0.23196888 0.08533677 0.08533677 0.08533677\n",
      " 0.08533677 0.08533693 0.08533679 0.08533677]\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:37.310145Z",
     "start_time": "2024-08-01T23:15:37.306551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(predictions)"
   ],
   "id": "467d033c7b2f9c5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 9 ... 3 9 2]\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:37.316315Z",
     "start_time": "2024-08-01T23:15:37.311292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Image ID column.\n",
    "image_id = []\n",
    "for x in range(1, test.shape[0] + 1):\n",
    "    image_id.append(x)"
   ],
   "id": "6b7f69d7cfd76172",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T23:15:37.336383Z",
     "start_time": "2024-08-01T23:15:37.317128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = pd.DataFrame({'ImageId': image_id, 'Label': predictions})\n",
    "\n",
    "model = 'cnn'\n",
    "now = datetime.now()\n",
    "date_time_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "output.to_csv(f'predictions/{model}_{date_time_str}.csv', index=False)"
   ],
   "id": "6a28d964a8d9d343",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results",
   "id": "bcfda3668cae32e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Got a 99.2% as the best result, final summary:\n",
    "\n",
    "- Using more epochs makes no sense, it just over fitting the model.\n",
    "- Data augmentation make not real impact so far, at least rotating images in this dataset make no significant improvement.\n",
    "- Convolutional Neuronal net got the results crazy good resutls.\n",
    "- Using the preferred model for the multiclass softmax appear to have been indifferent so far.\n",
    "- Model evaluate function for getting accuracy was pretty useful, also the confusion matriz to understand where we were doing worst. \n",
    "- Pending: play more with the CNN parameters once I understand them. \n"
   ],
   "id": "632f1886d8d5255d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6b72d6d480aeb9b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
